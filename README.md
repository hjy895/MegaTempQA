# MegaTempQA
MegaTempQA: A Large-Scale Dataset of 250 Million Temporal Question-Answer Pairs
# ğŸ•°ï¸ MegaTempQA: A Large-Scale Dataset of 250 Million Temporal Question-Answer Pairs

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Dataset](https://img.shields.io/badge/Dataset-250M_QA_Pairs-orange.svg)](data/)

## ğŸ“Š Overview

**MegaTempQA** is the largest temporal question answering dataset to date, containing over **250 million** carefully curated question-answer pairs designed to evaluate temporal reasoning capabilities in large language models.

### ğŸ¯ Key Features

- **ğŸ”¢ Scale**: 250+ million high-quality temporal QA pairs
- **ğŸ“… Coverage**: Spans multiple centuries of historical events
- **ğŸŒ Diversity**: Covers events, entities, and timelines across various domains
- **ğŸ§  Complexity**: 16 distinct question types for comprehensive evaluation
- **ğŸ“ˆ Multi-granular**: Supports temporal reasoning at different time scales
- **ğŸ”— Multi-hop**: Enables complex inferential reasoning tasks

### ğŸ“‹ Question Types

| **Attribute Questions** | **Comparison Questions** | **Complex Reasoning** |
|------------------------|-------------------------|----------------------|
| ğŸ¯ Event Attribute     | âš–ï¸ Event Comparison     | ğŸ§  Causal Reasoning  |
| ğŸ‘¤ Entity Attribute    | ğŸ‘¥ Entity Comparison    | â±ï¸ Duration Estimation |
| ğŸ• Time Attribute      | ğŸ“Š Time Comparison      | ğŸ“ Sequence Ordering |
| **Counting Questions** | **Temporal Analysis**    | **Advanced Reasoning** |
| ğŸ“Š Event Counting      | ğŸ”— Temporal Clustering  | ğŸŒ Cross-Domain      |
| ğŸ‘¥ Entity Counting     | ğŸ“ Multi-Granular       | ğŸ¤” Counterfactual    |
|                        | ğŸ”„ Temporal Overlap     |                      |


## ğŸ“ Repository Structure

```
MegaTempQA/
â”œâ”€â”€ ğŸ“œ README.md                    # Main documentation
â”œâ”€â”€ ğŸ“‹ requirements.txt             # Dependencies
â”œâ”€â”€ ğŸ“„ LICENSE                      # MIT License
â”œâ”€â”€ âš™ï¸  setup.py                     # Package setup
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“Š paper/                        # Paper materials
â”‚   â””â”€â”€ abstract.md                 # Paper abstract
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ”§ src/                          # Source code
â”‚   â”œâ”€â”€ generate_dataset.py         # Main dataset generator
â”‚   â”œâ”€â”€ evaluate_models.py          # Main evaluation script
â”‚   â”‚
â”‚   â”œâ”€â”€ data_generation/             # Dataset generation modules
â”‚   â”‚   â”œâ”€â”€ generator.py             # Core generator
â”‚   â”‚   â”œâ”€â”€ knowledge_base.py        # Historical data
â”‚   â”‚   â”œâ”€â”€ question_types.py        # Question type definitions
â”‚   â”‚   â”œâ”€â”€ validators.py            # Quality validation
â”‚   â”‚   â””â”€â”€ templates.py             # Question templates
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                  # Model evaluation modules
â”‚   â”‚   â”œâ”€â”€ evaluator.py             # Main evaluator
â”‚   â”‚   â”œâ”€â”€ model_manager.py         # Model loading/management
â”‚   â”‚   â”œâ”€â”€ metrics.py               # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py        # Few-shot prompts
â”‚   â”‚   â””â”€â”€ result_analyzer.py       # Results analysis
â”‚   â”‚
â”‚   â””â”€â”€ utils/                       # Utility modules
â”‚       â”œâ”€â”€ config.py                # Configuration
â”‚       â”œâ”€â”€ file_utils.py            # File operations
â”‚       â””â”€â”€ logging_utils.py         # Logging setup
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“Š data/                         # Dataset files
â”‚   â”œâ”€â”€ README.md                    # Data documentation
â”‚   â”œâ”€â”€ sample_dataset.csv           # Sample for testing
â”‚   â””â”€â”€ schema.json                  # Dataset schema
â”œâ”€â”€ 
â””â”€â”€ ğŸ§ª tests/                        # Test suite
    â””â”€â”€ test_generation.py           # Generation tests
```

## ğŸ“Š Dataset Statistics

| Metric | Value |
|--------|-------|
| **Total Questions** | 250+ Million |
| **Question Types** | 16 Categories |
| **Time Range** | 1900-2025 |
| **Domains** | History, Science, Technology, Politics, Culture |
| **Languages** | English |
| **Format** | CSV, JSON |

## ğŸ¯ Evaluation Results

Our comprehensive evaluation demonstrates:
- **Baseline Performance**: Most models struggle with temporal reasoning
- **Few-shot Learning**: 1-3 shot examples significantly improve performance  
- **Model Variations**: Significant performance differences across model families
- **Question Complexity**: Performance varies by temporal question type

## ğŸ“– Citation

```bibtex
@dataset{megatempqa2025,
  title={MegaTempQA: A Large-Scale Dataset of 250 Million Temporal Question-Answer Pairs for Language Models},
  year={2025},
  publisher={GitHub},
  url={https://github.com/yourusername/MegaTempQA}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please feel free to submit issues and enhancement requests.
